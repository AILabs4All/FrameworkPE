services:
  # Serviço Ollama para hospedar modelos SLM
  ollama:
    image: ollama/ollama:latest
    container_name: security-framework-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 1m
    deploy:
      resources:
        reservations:
          memory: 4G
        limits:
          memory: 8G

  # Serviço da aplicação Framework
  framework:
    build: .
    container_name: security-framework-app
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./results:/app/results
      - ./logs:/app/logs
      - ./config:/app/config
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
    working_dir: /app
    stdin_open: true
    tty: true
    restart: "no"

  # Serviço para inicializar modelos Ollama
  model-setup:
    build: .
    container_name: security-framework-setup
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./scripts:/app/scripts
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    command: ["bash", "/app/scripts/setup_ollama_models.sh"]
    restart: "no"

volumes:
  ollama_data:
    driver: local

networks:
  default:
    name: security-framework-network