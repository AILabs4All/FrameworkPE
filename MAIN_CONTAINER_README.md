# ğŸ³ Container para Experimentos main.py

Script para executar experimentos usando **main.py** em container Docker, aproveitando o container Ollama que jÃ¡ estÃ¡ rodando.

## ğŸ¯ PrÃ©-requisitos

VocÃª jÃ¡ deve ter o Ollama rodando:
```bash
docker ps | grep ollama  # Deve mostrar container ollama rodando
```

## ğŸš€ Uso RÃ¡pido

```bash
# 1. Testar se tudo estÃ¡ funcionando
./test-main-container.sh

# 2. Ver modelos disponÃ­veis
./run-main-experiment.sh --list-models

# 3. Simular experimento (dry-run)
./run-main-experiment.sh --dry-run

# 4. Executar experimento especÃ­fico
./run-main-experiment.sh --single ollama_mistral_7b progressive_hint

# 5. Executar experimento completo (108 classificaÃ§Ãµes)
./run-main-experiment.sh --full
```

## ğŸ“‹ Comandos DisponÃ­veis

### Experimentos

```bash
# Experimento completo (todos os 27 modelos Ã— 4 tÃ©cnicas)
./run-main-experiment.sh --full

# Experimento especÃ­fico
./run-main-experiment.sh --single <modelo> <tÃ©cnica>

# Exemplos de experimentos especÃ­ficos:
./run-main-experiment.sh --single ollama_mistral_7b progressive_hint
./run-main-experiment.sh --single ollama_falcon3_10b self_hint
./run-main-experiment.sh --single ollama_qwen2_7b hypothesis_testing
```

### UtilitÃ¡rios

```bash
# Simular execuÃ§Ã£o (ver comandos)
./run-main-experiment.sh --dry-run

# Listar modelos disponÃ­veis
./run-main-experiment.sh --list-models

# Apenas construir a imagem
./run-main-experiment.sh --build

# Ajuda
./run-main-experiment.sh --help
```

## ğŸ”§ Como Funciona

1. **Container Ollama**: Usa seu container Ollama existente via `--network host`
2. **Container Framework**: Cria um container temporÃ¡rio para cada execuÃ§Ã£o
3. **Volumes**: Monta diretÃ³rios `data/`, `results/`, `logs/`, `config/`
4. **ExecuÃ§Ã£o**: Roda `python main.py` com os parÃ¢metros corretos

## ğŸ“Š Estrutura do Experimento

### Comando Executado
```bash
python main.py data/ --columns target --model <modelo> --technique <tÃ©cnica> --output xlsx
```

### Modelos Testados (27 total)
- DeepSeek: `ollama_deepseek_15b`, `ollama_deepseek_r1_14b`, etc.
- Falcon: `ollama_falcon3_10b`, `ollama_falcon3_7b`, etc.
- Gemma: `ollama_gemma2_27b`, `ollama_gemma2_9b`, etc.
- Llama: `ollama_llama32_3b`, `ollama_llama33_70b`, etc.
- Mistral: `ollama_mistral_7b`, `ollama_mistral_large`, etc.
- Phi: `ollama_phi3_14b`, `ollama_phi3_mini`, etc.
- Qwen: `ollama_qwen2_7b`, `ollama_qwen2_5_32b`, etc.

### TÃ©cnicas de Prompt (4 total)
- `progressive_hint` - Dicas progressivas
- `progressive_rectification` - CorreÃ§Ã£o progressiva
- `self_hint` - Auto-sugestÃ£o
- `hypothesis_testing` - Teste de hipÃ³teses

## ğŸ“ Resultados

```
results/
â”œâ”€â”€ classification_results_YYYYMMDD_HHMMSS.xlsx
â”œâ”€â”€ progressive_hint/
â”‚   â”œâ”€â”€ ollama_mistral_7b_results.xlsx
â”‚   â””â”€â”€ ...
â”œâ”€â”€ progressive_rectification/
â”œâ”€â”€ self_hint/
â””â”€â”€ hypothesis_testing/

logs/
â”œâ”€â”€ framework_YYYYMMDD.log
â””â”€â”€ classification_errors.log
```

## ğŸ³ Detalhes TÃ©cnicos

### Dockerfile
- **Base**: `python:3.11-slim`
- **DependÃªncias**: Apenas essenciais (`curl`, `jq`, Python packages)
- **Otimizado**: Para execuÃ§Ã£o rÃ¡pida, sem ferramentas de compilaÃ§Ã£o

### Container
- **Network**: `--network host` (acessa Ollama localhost:11434)
- **Volumes**: Monta diretÃ³rios locais para persistir resultados
- **TemporÃ¡rio**: `--rm` remove container apÃ³s execuÃ§Ã£o
- **VariÃ¡veis**: `OLLAMA_BASE_URL=http://localhost:11434`

## âš¡ Exemplos PrÃ¡ticos

### Teste RÃ¡pido
```bash
# Testar um modelo especÃ­fico
./run-main-experiment.sh --single ollama_mistral_7b progressive_hint
```

### Experimento Parcial
```bash
# Executar alguns modelos manualmente
./run-main-experiment.sh --single ollama_mistral_7b progressive_hint
./run-main-experiment.sh --single ollama_mistral_7b self_hint
./run-main-experiment.sh --single ollama_falcon3_10b progressive_hint
```

### Experimento Completo
```bash
# Executar todos os 108 experimentos (4-8 horas)
./run-main-experiment.sh --full
```

## ğŸ” Monitoramento

```bash
# Ver resultados em tempo real
ls -la results/

# Ver logs
tail -f logs/framework_*.log

# Verificar recursos
docker stats
```

## ğŸ› ï¸ SoluÃ§Ã£o de Problemas

### Problema: Ollama nÃ£o encontrado
```bash
# Verificar se Ollama estÃ¡ rodando
docker ps | grep ollama

# Se nÃ£o estiver, iniciar:
docker run -d -p 11434:11434 --name ollama ollama/ollama
```

### Problema: Erro de rede
```bash
# Testar conectividade
curl http://localhost:11434/api/tags

# Verificar se porta estÃ¡ ocupada
netstat -tlnp | grep 11434
```

### Problema: Falta de dados
```bash
# Verificar arquivos de dados
ls -la data/

# Deve ter pelo menos um arquivo .xlsx
```

## ğŸ’¡ Vantagens

- âœ… **Reutiliza Ollama**: Usa container Ollama existente
- âœ… **Container Simples**: Sem complexidade de Docker Compose
- âœ… **ExecuÃ§Ã£o RÃ¡pida**: Container temporÃ¡rio para cada experimento
- âœ… **FlexÃ­vel**: Pode executar experimentos especÃ­ficos ou completos
- âœ… **Logs Detalhados**: Acompanha progresso e erros
- âœ… **FÃ¡cil Debug**: Cada execuÃ§Ã£o Ã© independente

Execute `./test-main-container.sh` para verificar se tudo estÃ¡ funcionando! ğŸš€